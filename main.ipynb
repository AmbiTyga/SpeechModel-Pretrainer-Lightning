{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    get_scheduler\n",
    ")\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    _compute_mask_indices, _sample_negative_indices\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "import lightning as L\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Optional, Dict\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    mask_time_prob: Optional[float] = 0.65\n",
    "    mask_time_length: Optional[int] = 10\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.feature_extractor.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        device = batch['input_values'].device\n",
    "        batch_size = batch['input_values'].shape[0]\n",
    "\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(\n",
    "            batch[\"input_values\"].shape[-1]\n",
    "        )\n",
    "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
    "\n",
    "        if batch.get(\"attention_mask\") is not None:\n",
    "            batch['sub_attention_mask'] = self.model._get_feature_vector_attention_mask(\n",
    "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
    "            )\n",
    "        \n",
    "        features_shape = (batch_size, mask_indices_seq_length)\n",
    "\n",
    "        # Sample Randomly Masked Indices\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            features_shape,\n",
    "            self.mask_time_prob,\n",
    "            self.mask_time_length,\n",
    "            attention_mask=batch.get(\"sub_attention_mask\")\n",
    "        )\n",
    "\n",
    "        # Sample Negative indices\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape,\n",
    "            self.model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices\n",
    "        )\n",
    "\n",
    "        batch[\"mask_time_indices\"] = torch.tensor(\n",
    "            mask_time_indices, dtype=torch.long, device=device\n",
    "        )\n",
    "        batch['sampled_negative_indices'] = torch.tensor(\n",
    "            sampled_negative_indices, dtype=torch.long, device=device\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_grads(params, c):\n",
    "    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            if torch.is_tensor(c):\n",
    "                c = c.to(p.grad.device)\n",
    "            p.grad.data.mul_(c)\n",
    "\n",
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f892bfd5824eeeb7f7c2a6eaf575a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from pydub import AudioSegment\n",
    "audio_paths = []\n",
    "for audio_path in tqdm(glob(\"v1/*/v1a/train/*.wav\")):\n",
    "    if audio_path not in audio_paths:\n",
    "        audio_len = AudioSegment.from_file(audio_path).duration_seconds\n",
    "    if audio_len > 2 and audio_len < 20:\n",
    "        audio_paths.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filenames.data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(audio_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eb4bf2e0ee4c259bfcebc0e1b8798f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "audios = pd.DataFrame(open(\"filenames.data.txt\").readlines(), columns=['path'])\n",
    "audios['path'] = audios['path'].apply(lambda x: x.strip())\n",
    "from multiprocessing import Pool\n",
    "from tqdm.auto import tqdm\n",
    "def get_duration(x):\n",
    "    return AudioSegment.from_file(x).duration_seconds\n",
    "with Pool(8) as p:\n",
    "    audios['duration'] = list(tqdm(p.imap(get_duration, audios['path']), total=len(audios.path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios.sort_values('duration', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_index = int(len(audios) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = audios[:validation_index], audios[validation_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['path'].to_csv(\"train.data.txt\", header=False, index=False)\n",
    "val['path'].to_csv(\"validation.data.txt\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths):\n",
    "        self.audio_sf_paths = [wav.replace('.wav', '.safetensors') for wav in audio_paths]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_sf_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = load_file(self.audio_sf_paths[idx])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def gather_for_metrics(metric_tensor):\n",
    "    \"\"\"\n",
    "    Gathers and sums metrics across all processes in a distributed training environment.\n",
    "    \n",
    "    Args:\n",
    "    metric_tensor (torch.Tensor): A tensor containing the metric to aggregate.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The aggregated metric.\n",
    "    \"\"\"\n",
    "    if not dist.is_initialized():\n",
    "        raise RuntimeError(\"Distributed package is not initialized\")\n",
    "    \n",
    "    # Ensure the tensor is on the correct device\n",
    "    metric_tensor = metric_tensor.to(dtype=torch.float32)\n",
    "\n",
    "    # Use all_reduce to sum up all the metrics across all processes\n",
    "    dist.all_reduce(metric_tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    return metric_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Wav2Vec2PretrainingModule(L.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name: str,\n",
    "            datasets_path: List[str],\n",
    "            learning_rate: float = 1e-3, \n",
    "            batch_size_per_device: int = 2,\n",
    "            lr_warmup_steps: int = 10, \n",
    "            max_gumbel_temperature: float = 2.0,\n",
    "            min_gumbel_temperature: float = 0.5,\n",
    "            gumbel_temperature_decay: float = 0.999995\n",
    "    ):\n",
    "        super(Wav2Vec2PretrainingModule, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size_per_device\n",
    "        self.lr_warmup_steps = lr_warmup_steps\n",
    "        self.model_name = model_name\n",
    "\n",
    "        config = Wav2Vec2Config.from_pretrained(model_name)\n",
    "        self.mask_time_prob = config.mask_time_prob if config.mask_time_prob>0 else 0.65\n",
    "        self.mask_time_length = config.mask_time_length if config.mask_time_length>0 else 10  \n",
    "        \n",
    "        self.max_gumbel_temperature = max_gumbel_temperature\n",
    "        self.min_gumbel_temperature = min_gumbel_temperature\n",
    "        self.gumbel_temperature_decay = gumbel_temperature_decay\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Wav2Vec2ForPreTraining(config)\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "            model=self.model,\n",
    "            feature_extractor=feature_extractor,\n",
    "            mask_time_prob=self.mask_time_prob,\n",
    "            mask_time_length=self.mask_time_length\n",
    "        )\n",
    "\n",
    "        train_path, val_path = datasets_path\n",
    "        self.trainset = AudioDataset(pd.read_csv(train_path, header=None)[0].tolist())\n",
    "        self.valset = AudioDataset(pd.read_csv(val_path, header=None)[0].tolist())\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valset,\n",
    "            batch_size=self.batch_size*2,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        num_losses = batch['mask_time_indices'].sum()\n",
    "        sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
    "        sub_attention_mask = (\n",
    "            sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"masked_time_indices\"])\n",
    "        )\n",
    "\n",
    "        outputs = self.model(**batch)\n",
    "    \n",
    "        if self.trainer.num_nodes>1:\n",
    "            num_losses = gather_for_metrics(num_losses).sum()\n",
    "            gradient_multiplier = self.trainer.num_nodes / num_losses\n",
    "            multiply_grads(self.model.module.parameters(), gradient_multiplier)\n",
    "        else:\n",
    "            multiply_grads(self.model.parameters(), 1.0 / num_losses)\n",
    "        return outputs\n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch: Any, batch_idx: int) -> None:\n",
    "        # For logging\n",
    "        loss_log = outputs['loss'].detach()\n",
    "        contrastive_loss_log = outputs['contrastive_loss'].detach()\n",
    "        diversity_loss_log = outputs['diversity_loss'].detach()\n",
    "\n",
    "        if self.trainer.world_size > 1:\n",
    "            loss_log = gather_for_metrics(loss_log)\n",
    "            contrastive_loss_log = gather_for_metrics(contrastive_loss_log)\n",
    "            diversity_loss_log = gather_for_metrics(diversity_loss_log)\n",
    "        self.log('loss', loss_log, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('contrastive_loss', contrastive_loss_log, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('diversity_loss', diversity_loss_log, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch.pop(\"sub_attention_mask\", None)\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        contrastive_loss = outputs.contrastive_loss\n",
    "        diversity_loss = outputs.diversity_loss\n",
    "        if self.trainer.world_size > 1:\n",
    "            loss = gather_for_metrics(loss)\n",
    "            contrastive_loss = gather_for_metrics(contrastive_loss)\n",
    "            diversity_loss = gather_for_metrics(diversity_loss)\n",
    "\n",
    "        num_losses = batch['mask_time_indices'].sum()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log(\"val_contrastive_loss\", contrastive_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log(\"val_diversity_loss\", diversity_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log(\"val_num_losses\", num_losses, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            list(self.model.parameters()),\n",
    "            lr = self.learning_rate,\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-6,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer= optimizer,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "            num_warmup_steps=self.lr_warmup_steps\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer: Optimizer) -> None:\n",
    "        gumbel_temperature = max(\n",
    "                self.max_gumbel_temperature * self.gumbel_temperature_decay**(self.global_step%(self.batch_size*self.trainer.num_nodes)),\n",
    "                self.min_gumbel_temperature,\n",
    "            )\n",
    "        if hasattr(self.model, \"module\"):\n",
    "            self.model.module.set_gumbel_temperature(gumbel_temperature)\n",
    "        else:\n",
    "            self.model.set_gumbel_temperature(gumbel_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import os\n",
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--val_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--lr_warmup_steps\", type=int, default=32000)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"wav2vec2-indic-voices\")\n",
    "    parser.add_argument(\"--accelerator\", type=str, default=\"gpu\")\n",
    "    parser.add_argument(\"--devices\", type=int, default=-1)\n",
    "    parser.add_argument(\"--precision\", type=Any[str, int], default=16)\n",
    "    parser.add_argument('--training_steps', type=int, default=200000)\n",
    "    parser.add_argument(\"--accumulate_grad_batches\", type=int, default=8)\n",
    "    parser.add_argument(\"--gradient_clip_val\", type=float, default=8)\n",
    "\n",
    "    parser.add_argument(\"--max_gumbel_temperature\", type=float, default=2.0)\n",
    "    parser.add_argument(\"--min_gumbel_temperature\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--gumbel_temperature_decay\", type=float, default=0.999995)\n",
    "    parser.add_argument(\"--save_weights_only\", action=\"store_true\")\n",
    "    parser.add_argument(\"--save_every_n_steps\", type=int, default=10000)\n",
    "    return parser.parse_args()\n",
    "\n",
    "## Write CLI Command without comments hash to run this script only for those values that are required, make it multi line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2PretrainingModule(\n",
    "    model_name=\"facebook/wav2vec2-large-xlsr-53\",\n",
    "    datasets_path=[\"train.data.txt\", \"validation.data.txt\"],\n",
    "    learning_rate=1e-3,\n",
    "    batch_size_per_device=5,\n",
    "    lr_warmup_steps=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpting = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"wav2vec2-indic-voices\",\n",
    "    filename=\"{step:06}.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    every_n_train_steps=10000,\n",
    ")\n",
    "tensorboardlogger = L.pytorch.loggers.TensorBoardLogger(\n",
    "    save_dir=\"wav2vec2-indic-voices\",\n",
    "    name=\"wav2vec2-indic-voices-tb\",\n",
    "    version=0\n",
    ")\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=-1,\n",
    "    precision=16,\n",
    "    max_steps=200000,\n",
    "    num_sanity_val_steps=1,\n",
    "    logger=tensorboardlogger,\n",
    "    callbacks=[ckpting],\n",
    "    accumulate_grad_batches=8,\n",
    "    gradient_clip_val=8,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/hike-e2e/indicVoices/wav2vec2-indic-voices exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                   | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model | Wav2Vec2ForPreTraining | 317 M  | train\n",
      "---------------------------------------------------------\n",
      "317 M     Trainable params\n",
      "0         Non-trainable params\n",
      "317 M     Total params\n",
      "1,269.562 Total estimated model params size (MB)\n",
      "412       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c4daeaa9a74737bc68b740baf3b034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7706ab7b864507aba617953cb25777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hike-e2e/.conda3/envs/research-tts/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
